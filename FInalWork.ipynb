{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (51,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Convert to NumPy arrays\u001b[39;00m\n\u001b[1;32m     60\u001b[0m images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(images)\n\u001b[0;32m---> 61\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbounding_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Resize images and bounding boxes\u001b[39;00m\n\u001b[1;32m     64\u001b[0m resized_images \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (51,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset, random_split\n",
    "# Other necessary imports for your specific dataset and preprocessing\n",
    "import os, time, glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import matplotlib.patches as patches\n",
    "from torchvision import ops\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_folder = '/home/m3-learning/Documents/myML/preprocessed_data/'\n",
    "file_list = os.listdir(data_folder)\n",
    "\n",
    "file_pairs = {}\n",
    "for file in file_list:\n",
    "    if file.endswith('_img.npz'):\n",
    "        identifier = file.split('_')[0]\n",
    "        if identifier not in file_pairs:\n",
    "            file_pairs[identifier] = {'img': file, 'box': None}\n",
    "        else:\n",
    "            file_pairs[identifier]['img'] = file\n",
    "    elif file.endswith('_box.npz'):\n",
    "        identifier = file.split('_')[0]\n",
    "        if identifier not in file_pairs:\n",
    "            file_pairs[identifier] = {'img': None, 'box': file}\n",
    "        else:\n",
    "            file_pairs[identifier]['box'] = file\n",
    "\n",
    "# Load image and bounding box data\n",
    "images = []\n",
    "bounding_boxes = []\n",
    "\n",
    "for identifier, files in file_pairs.items():\n",
    "    img_file = files['img']\n",
    "    box_file = files['box']\n",
    "    if img_file and box_file:\n",
    "        img_data = np.load(os.path.join(data_folder, img_file))['arr_0']\n",
    "        box_data = np.load(os.path.join(data_folder, box_file))['arr_0']\n",
    "        images.append(img_data)\n",
    "        bounding_boxes.append(box_data)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "images = np.array(images)\n",
    "bounding_boxes = np.array(bounding_boxes)\n",
    "\n",
    "# Resize images and bounding boxes\n",
    "resized_images = []\n",
    "resized_bounding_boxes = []\n",
    "\n",
    "target_height = 100  # Set your desired height\n",
    "target_width = 100   # Set your desired width\n",
    "\n",
    "\n",
    "for image, bounding_box in zip(images, bounding_boxes):\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (target_width, target_height))  # Using OpenCV for resizing\n",
    "    resized_images.append(resized_image)\n",
    "    \n",
    "    # Calculate the scaling factor for bounding boxes (assuming resizing is proportional)\n",
    "    height_scale = target_height / image.shape[0]\n",
    "    width_scale = target_width / image.shape[1]\n",
    "    \n",
    "    # Scale bounding box coordinates accordingly\n",
    "    scaled_bounding_box = bounding_box * np.array([width_scale, height_scale, width_scale, height_scale])\n",
    "    \n",
    "    # Ensure the bounding box has the same number of elements (i.e., a consistent shape)\n",
    "    if scaled_bounding_box.shape == (4,):\n",
    "        resized_bounding_boxes.append(scaled_bounding_box)\n",
    "    else:\n",
    "        print(f\"Ignoring bounding box: {scaled_bounding_box} with shape: {scaled_bounding_box.shape}\")\n",
    "\n",
    "\n",
    "# Convert the resized images and bounding boxes back to numpy arrays\n",
    "resized_images = np.array(resized_images)\n",
    "resized_bounding_boxes = np.array(resized_bounding_boxes)\n",
    "\n",
    "# Continue with the rest of your code using resized_images and resized_bounding_boxes\n",
    "\n",
    "# Define the model\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_shape,num_boxes = 272, num_classes = 4):\n",
    "        super(FCNN, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 8 * 8, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, (num_boxes * num_classes))  # Output layer for multiple boxes\n",
    "        )\n",
    "\n",
    "        self.num_boxes = num_boxes\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.view(-1, self.num_boxes, self.num_classes)\n",
    "\n",
    "input_channels = 1\n",
    "input_height = 64\n",
    "input_width = 64\n",
    "\n",
    "# Define the input size based on the input image shape\n",
    "input_shape = input_channels * input_height * input_width\n",
    "\n",
    "# Define your custom dataset\n",
    "class BoundingBoxDataset(Dataset):\n",
    "    def __init__(self, images, bounding_boxes, transform):\n",
    "        self.images = images\n",
    "        self.bounding_boxes = bounding_boxes\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        bounding_box = self.bounding_boxes[idx]\n",
    "        \n",
    "        \n",
    "\n",
    "        return image, bounding_box\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_images, test_images, train_bounding_boxes, test_bounding_boxes = train_test_split(\n",
    "    resized_images, resized_bounding_boxes, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "train_images_tensor = torch.from_numpy(train_images).float()\n",
    "train_bounding_boxes_tensor = torch.from_numpy(train_bounding_boxes).float()\n",
    "\n",
    "test_images_tensor = torch.from_numpy(test_images).float()\n",
    "test_bounding_boxes_tensor = torch.from_numpy(test_bounding_boxes).float()\n",
    "\n",
    "# Add a channel dimension to grayscale images\n",
    "train_images_tensor = train_images_tensor.unsqueeze(1)  # Adds a channel dimension at index 1\n",
    "test_images_tensor = test_images_tensor.unsqueeze(1)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create instances of your custom dataset\n",
    "train_dataset = BoundingBoxDataset(train_images_tensor, train_bounding_boxes_tensor,transform=transform)\n",
    "test_dataset = BoundingBoxDataset(test_images_tensor, test_bounding_boxes_tensor,transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "fcnn_model = FCNN(input_shape, num_classes=4)\n",
    "\n",
    "\n",
    "\n",
    "class MinMaxScaler:\n",
    "    def __init__(self):\n",
    "        self.min = None\n",
    "        self.max = None\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.min = torch.min(data, dim=0)[0]\n",
    "        self.max = torch.max(data, dim=0)[0]\n",
    "    \n",
    "    def transform(self, data):\n",
    "        return (data - self.min) / (self.max - self.min)\n",
    "    \n",
    "    def fit_transform(self, data):\n",
    "        self.fit(data)\n",
    "        return self.transform(data)\n",
    "\n",
    "# Define your loss function and optimizer and Scaler\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(fcnn_model.parameters(), lr=0.001)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "num_epochs = 60\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    fcnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, bounding_boxes in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # No need to flatten the input, keep it as 4D tensor [batch_size, channels, height, width]\n",
    "        outputs = fcnn_model(images)\n",
    "        \n",
    "        loss = criterion(outputs, bounding_boxes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print predicted bounding box parameters for each batch during training\n",
    "        print(f\"Predicted Bounding Box Parameters (Training):\\n{outputs[0]}\")  # Modify this according to your output structure\n",
    "\n",
    "    # Print training loss for each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation\n",
    "fcnn_model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, bounding_boxes in test_loader:\n",
    "        outputs = fcnn_model(images)\n",
    "        test_loss += criterion(outputs, bounding_boxes).item()\n",
    "        \n",
    "        # Print predicted bounding box parameters for each batch during testing\n",
    "        print(f\"Predicted Bounding Box Parameters (Testing):\\n{outputs[0]}\")  # Modify this according to your output structure\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predicted Bounding Boxes\n",
    "Blue is ground truth,\n",
    "Red is predicted Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with visualization\n",
    "fcnn_model.eval()\n",
    "\n",
    "# Get a batch of test data\n",
    "images, bounding_boxes = next(iter(test_loader))\n",
    "\n",
    "# Perform prediction\n",
    "with torch.no_grad():\n",
    "    outputs = fcnn_model(images)\n",
    "\n",
    "# Plot predicted and ground truth bounding boxes on images\n",
    "num_samples = 1  # Number of samples to visualize\n",
    "fig, axes = plt.subplots(nrows=num_samples, ncols=1, figsize=(8, 12))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Convert tensors to numpy arrays\n",
    "    img_np = images[i].squeeze().numpy()\n",
    "    pred_box = outputs[i].numpy()\n",
    "    true_box = bounding_boxes[i].squeeze().numpy()\n",
    "\n",
    "    # Display the image\n",
    "    ax = axes if num_samples == 1 else axes[i]\n",
    "    ax.imshow(img_np, cmap='gray')\n",
    "    ax.set_title(f'Sample {i + 1}')\n",
    "\n",
    "    # Plot predicted box in red\n",
    "    pred_rect = patches.Rectangle((pred_box[0], pred_box[1]), pred_box[2], pred_box[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(pred_rect)\n",
    "\n",
    "    # Plot ground truth box in green\n",
    "    true_rect = patches.Rectangle((true_box[0], true_box[1]), true_box[2], true_box[3], linewidth=2, edgecolor='g', facecolor='none')\n",
    "    ax.add_patch(true_rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "electron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
